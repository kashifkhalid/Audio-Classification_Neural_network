{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import pickle\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# needed for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "current_palette = sns.color_palette()\n",
    "import pdb\n",
    "from numpy import array\n",
    "import random as rd\n",
    "import sys\n",
    "#reload(sys)\n",
    "#sys.setdefaultencoding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.2.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Requires TensorFlow V.12\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getkey(item):\n",
    "    fpr, tpr, _ = roc_curve(test_labels.ravel(), test_preds[item])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    return roc_auc\n",
    "\n",
    "def plotROC(testlabels, test_preds):\n",
    "    classifiers = list(test_preds.keys())\n",
    "\n",
    "    # Plot all ROC curves\n",
    "    plt.figure(figsize=(15,9))\n",
    "    for i, clf in zip(range(len(classifiers)), sorted(classifiers, key=getkey, reverse=True)):\n",
    "        fpr, tpr, _ = roc_curve(testlabels.ravel(), test_preds[clf] )\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr,\n",
    "                 label='ROC curve '+ clf +  ' (area = {0:0.4f})'\n",
    "                       ''.format(roc_auc), linestyle='-', linewidth=2)\n",
    "\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([-0.1, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Comparison of multiclass ROC curves')\n",
    "    plt.legend(loc=\"lower right\", fontsize=14)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shape</th>\n",
       "      <th>label</th>\n",
       "      <th>music</th>\n",
       "      <th>noise</th>\n",
       "      <th>silence</th>\n",
       "      <th>speech</th>\n",
       "      <th>sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3004</th>\n",
       "      <td>(1, 193)</td>\n",
       "      <td>noise</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[-223.41727359645648, 99.13116797039905, -57.5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3005</th>\n",
       "      <td>(1, 193)</td>\n",
       "      <td>noise</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[-213.539618903399, 100.16073068189185, -50.19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3006</th>\n",
       "      <td>(1, 193)</td>\n",
       "      <td>noise</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[-219.8521240138423, 96.86084396902716, -60.07...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3007</th>\n",
       "      <td>(1, 193)</td>\n",
       "      <td>noise</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[-226.19836233959882, 88.05429771807319, -69.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3008</th>\n",
       "      <td>(1, 193)</td>\n",
       "      <td>noise</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[-351.9375465095595, 108.25304673045771, -40.4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         shape  label  music  noise  silence  speech  \\\n",
       "3004  (1, 193)  noise    0.0    1.0      0.0     0.0   \n",
       "3005  (1, 193)  noise    0.0    1.0      0.0     0.0   \n",
       "3006  (1, 193)  noise    0.0    1.0      0.0     0.0   \n",
       "3007  (1, 193)  noise    0.0    1.0      0.0     0.0   \n",
       "3008  (1, 193)  noise    0.0    1.0      0.0     0.0   \n",
       "\n",
       "                                                 sample  \n",
       "3004  [-223.41727359645648, 99.13116797039905, -57.5...  \n",
       "3005  [-213.539618903399, 100.16073068189185, -50.19...  \n",
       "3006  [-219.8521240138423, 96.86084396902716, -60.07...  \n",
       "3007  [-226.19836233959882, 88.05429771807319, -69.2...  \n",
       "3008  [-351.9375465095595, 108.25304673045771, -40.4...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pickle.load(open('features_file.p', 'rb'))\n",
    "data.tail()\n",
    "\n",
    "#print (data[2000:4000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['shape', 'label', 'music', 'noise', 'silence', 'speech', 'sample'], dtype='object')\n",
      "(3009, 7)\n"
     ]
    }
   ],
   "source": [
    "#pdb.set_trace()\n",
    "print(data.columns)\n",
    "\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working dataframe's shape: (3009, 194)\n",
      "0       music\n",
      "1       music\n",
      "2       music\n",
      "3       music\n",
      "4       music\n",
      "5       music\n",
      "6       music\n",
      "7       music\n",
      "8       music\n",
      "9       music\n",
      "10      music\n",
      "11      music\n",
      "12      music\n",
      "13      music\n",
      "14      music\n",
      "15      music\n",
      "16      music\n",
      "17      music\n",
      "18      music\n",
      "19      music\n",
      "20      music\n",
      "21      music\n",
      "22      music\n",
      "23      music\n",
      "24      music\n",
      "25      music\n",
      "26      music\n",
      "27      music\n",
      "28      music\n",
      "29      music\n",
      "        ...  \n",
      "2979    noise\n",
      "2980    noise\n",
      "2981    noise\n",
      "2982    noise\n",
      "2983    noise\n",
      "2984    noise\n",
      "2985    noise\n",
      "2986    noise\n",
      "2987    noise\n",
      "2988    noise\n",
      "2989    noise\n",
      "2990    noise\n",
      "2991    noise\n",
      "2992    noise\n",
      "2993    noise\n",
      "2994    noise\n",
      "2995    noise\n",
      "2996    noise\n",
      "2997    noise\n",
      "2998    noise\n",
      "2999    noise\n",
      "3000    noise\n",
      "3001    noise\n",
      "3002    noise\n",
      "3003    noise\n",
      "3004    noise\n",
      "3005    noise\n",
      "3006    noise\n",
      "3007    noise\n",
      "3008    noise\n",
      "Name: label, Length: 3009, dtype: object\n"
     ]
    }
   ],
   "source": [
    "s = list(data['sample'])\n",
    "s = pd.DataFrame(s)\n",
    "data_cols = s.columns\n",
    "s['label'] = data['label']\n",
    "print('working dataframe\\'s shape:', s.shape)\n",
    "#pdb.set_trace()\n",
    "print(s['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kkhalid\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "test_preds = {}\n",
    "#train=s[0:3500]\n",
    "#test=s[3500:7035]\n",
    "train =s.sample(frac=0.7)#data division 70% randomly picked samples for training\n",
    "test = s.loc[~s.index.isin(train.index)]#remaining 30% randomly picked samples\n",
    "LB = LabelBinarizer().fit(train['label'])\n",
    "test_labels = LB.transform(test['label'])\n",
    "scaler1 = sk.preprocessing.StandardScaler().fit(train.loc[:,data_cols])\n",
    "train.loc[:,data_cols] = scaler1.transform(train.loc[:,data_cols])\n",
    "test.loc[:,data_cols] = scaler1.transform(test.loc[:,data_cols])\n",
    "#print(test.loc)\n",
    "#print(LB)\n",
    "print(test_labels[200:400])\n",
    "# truncate data\n",
    "#data_cols = data_cols[:1000]\n",
    "#pdb.set_trace()\n",
    "# print shapes\n",
    "#print('train shape {}\\ntest  shape {}\\ncommon factors: 1, 2, 3, 6, 9, 18, 97, 194, 291, 582, 873, 1746'.format(train.shape, test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pdb.set_trace()\n",
    "\n",
    "del data, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "            / predictions.shape[0])\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.01)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def test_accuracy(session, test_data=test, during = True):\n",
    "    test_data.reset_index(inplace=True, drop=True)\n",
    "    epoch_pred = session.run(test_prediction, feed_dict={tf_data : test_data.loc[0:check_size-1,data_cols], keep_prob : 1.0})\n",
    "    for i in range(check_size, test_data.shape[0], check_size):\n",
    "        epoch_pred = np.concatenate([epoch_pred, session.run(test_prediction, \n",
    "                                    feed_dict={tf_data : test_data.loc[i:i+check_size-1,data_cols], keep_prob : 1.0})], axis=0)\n",
    "    if during:\n",
    "        return accuracy(epoch_pred, test_labels)\n",
    "    else:\n",
    "        return epoch_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = 4\n",
    "batch_size = 97\n",
    "acc_over_time = {}\n",
    "def Run_Session(num_epochs, name, k_prob=1.0, mute=False, record=False):\n",
    "    global train\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    start = timer()\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        if record:\n",
    "            merged = tf.merge_all_summaries()  \n",
    "            writer = tf.train.SummaryWriter(\"/tmp/tensorflowlogs\", session.graph)\n",
    "        #tf.initialize_all_variables().run()\n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "        print(\"Initialized\")\n",
    "        \n",
    "        saver=tf.train.Saver()\n",
    "        save_dir='C:/Users/kkhalid/project_kk'\n",
    "        save_path = os.path.join(save_dir, 'CNN_model_trained')\n",
    "        #pdb.set_trace()\n",
    "        \n",
    "        accu = []\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            \n",
    "            # get batch\n",
    "            train_batch = train.sample(batch_size)\n",
    "            \n",
    "            t_d = train_batch[data_cols]\n",
    "            t_l = LB.transform(train_batch['label'])\n",
    "            \n",
    "            # make feed dict\n",
    "            feed_dict = { tf_data : t_d, train_labels : t_l, keep_prob : k_prob}\n",
    "            \n",
    "            \n",
    "           \n",
    "            #pdb.set_trace()\n",
    "            # run model on batch\n",
    "            _, l, predictions = session.run([optimizer, loss, prediction], feed_dict=feed_dict)\n",
    "            \n",
    "            #saver.save(sess=session, save_path=save_path)\n",
    "\n",
    "\n",
    "\n",
    "            #print(predictions)\n",
    "            #print(loss)\n",
    "            #pdb.set_trace()\n",
    "            # mid model accuracy checks \n",
    "            if (epoch % 1000 == 0) and not mute:\n",
    "                print(\"\\tMinibatch loss at epoch {}: {}\".format(epoch, l))\n",
    "                print(\"\\tMinibatch accuracy: {:.1f}\".format(accuracy(predictions, t_l)))\n",
    "            if (epoch % 1000 == 0) and not mute:\n",
    "                print(\"Test accuracy: {:.1f}\".format(test_accuracy(session, during=True)))\n",
    "            if (epoch % 1000 == 0) and not mute:\n",
    "                accu.append(tuple([epoch, test_accuracy(session, during=True)]))\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "        #saver=tf.train.Saver()\n",
    "        saver.save(sess=session, save_path=save_path)\n",
    "     \n",
    "        \n",
    "        #saver.restore(session,tf.train.latest_checkpoint(\"C:/Users/kkhalid/project_kk/model-ckpt/checkpoint\"))\n",
    "        \n",
    "        #print(session.run('w1:0'))\n",
    "        # record accuracy and predictions\n",
    "        \n",
    "        test_preds[name] = test_accuracy(session, during=False)\n",
    "        print(\"Final Test accuracy: {:.1f}\".format(accuracy(test_preds[name], test_labels)))\n",
    "        end = timer()\n",
    "        test_preds[name] = test_preds[name].ravel()\n",
    "        acc_over_time[name] = accu\n",
    "        print(\"time taken: {0} minutes {1:.1f} seconds\".format((end - start)//60, (end - start)%60))\n",
    "        #tf.train.export_meta_graph()\n",
    "        #saver=tf.train.import_meta_graph('C:/Users/kkhalid/project_kk/model-ckpt/trained_py_variation_6k.ckpt.meta')\n",
    "        \n",
    "        #saver.restore(session,tf.train.latest_checkpoint(\"C:/Users/kkhalid/project_kk/model-ckpt/checkpoint\"))\n",
    "        #saver.restore(sess=session, save_path=save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic DeepNN model made\n"
     ]
    }
   ],
   "source": [
    "# constants\n",
    "num_labels = 4\n",
    "\n",
    "batch_size = 97\n",
    "check_size = 582\n",
    "feature_size = 193\n",
    "\n",
    "n_hidden1 = 800\n",
    "n_hidden2 = 1000\n",
    "n_hidden3 = 1000\n",
    "n_hidden4 = 800\n",
    "\n",
    "beta = 0.01\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # placeholders\n",
    "    tf_data = tf.placeholder(tf.float32, shape=[None, feature_size],name='input_data')\n",
    "    train_labels = tf.placeholder(tf.float32, shape=[None,num_labels],name='input_labels')\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    #pdb.set_trace()\n",
    "    # weights and biases\n",
    "    layer1_weights = weight_variable([feature_size, n_hidden1])\n",
    "    layer1_biases = bias_variable([n_hidden1])\n",
    "    layer2_weights = weight_variable([n_hidden1, n_hidden2])\n",
    "    layer2_biases = bias_variable([n_hidden2])\n",
    "    layer3_weights = weight_variable([n_hidden2, n_hidden3])\n",
    "    layer3_biases = bias_variable([n_hidden3])\n",
    "    layer4_weights = weight_variable([n_hidden3, n_hidden4])\n",
    "    layer4_biases = bias_variable([n_hidden4])\n",
    "    layer5_weights = weight_variable([n_hidden4, num_labels])\n",
    "    layer5_biases = bias_variable([num_labels])\n",
    "\n",
    "     # model\n",
    "    def model(data, proba=0.8):\n",
    "        layer1 = tf.nn.relu(tf.matmul(data, layer1_weights) + layer1_biases)\n",
    "        layer1 = tf.nn.dropout(layer1, proba)\n",
    "        \n",
    "        layer2 = tf.nn.relu(tf.matmul(layer1, layer2_weights) + layer2_biases)\n",
    "        layer2 = tf.nn.dropout(layer2, proba)\n",
    "        \n",
    "        layer3 = tf.nn.relu(tf.matmul(layer2, layer3_weights) + layer3_biases)\n",
    "        layer3 = tf.nn.dropout(layer3, proba)\n",
    "        \n",
    "        layer4= tf.nn.relu(tf.matmul(layer3, layer4_weights) + layer4_biases)\n",
    "        layer4 = tf.nn.dropout(layer4, proba)\n",
    "        \n",
    "        return tf.matmul(layer4, layer5_weights) + layer5_biases\n",
    "\n",
    "    # Training computation.\n",
    "    logits = model(tf_data, keep_prob)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=train_labels) +\n",
    "                         beta*tf.nn.l2_loss(layer1_weights) +\n",
    "                         beta*tf.nn.l2_loss(layer1_biases) +\n",
    "                         beta*tf.nn.l2_loss(layer2_weights) +\n",
    "                         beta*tf.nn.l2_loss(layer2_biases) +\n",
    "                         beta*tf.nn.l2_loss(layer3_weights) +\n",
    "                         beta*tf.nn.l2_loss(layer3_biases) +\n",
    "                         beta*tf.nn.l2_loss(layer4_weights) +\n",
    "                         beta*tf.nn.l2_loss(layer4_biases) +\n",
    "                         beta*tf.nn.l2_loss(layer5_weights) +\n",
    "                         beta*tf.nn.l2_loss(layer5_biases))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "   \n",
    "\n",
    "\n",
    "    prediction = tf.nn.softmax(logits,name='op_to_restore')\n",
    "    test_prediction = tf.nn.softmax(model(tf_data, proba=0.7),name='op_to_restore')  \n",
    "    print('Basic DeepNN model made')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "\tMinibatch loss at epoch 0: 2.6360561847686768\n",
      "\tMinibatch accuracy: 21.6\n",
      "Test accuracy: 24.0\n",
      "\tMinibatch loss at epoch 1000: 0.5142006874084473\n",
      "\tMinibatch accuracy: 89.7\n",
      "Test accuracy: 87.7\n",
      "\tMinibatch loss at epoch 2000: 0.3138744831085205\n",
      "\tMinibatch accuracy: 97.9\n",
      "Test accuracy: 89.0\n",
      "\tMinibatch loss at epoch 3000: 0.378925085067749\n",
      "\tMinibatch accuracy: 94.8\n",
      "Test accuracy: 88.8\n",
      "\tMinibatch loss at epoch 4000: 0.3511411249637604\n",
      "\tMinibatch accuracy: 96.9\n",
      "Test accuracy: 88.5\n",
      "\tMinibatch loss at epoch 5000: 0.28055280447006226\n",
      "\tMinibatch accuracy: 96.9\n",
      "Test accuracy: 88.9\n",
      "\tMinibatch loss at epoch 6000: 0.3175709843635559\n",
      "\tMinibatch accuracy: 97.9\n",
      "Test accuracy: 88.5\n",
      "\tMinibatch loss at epoch 7000: 0.2723849415779114\n",
      "\tMinibatch accuracy: 97.9\n",
      "Test accuracy: 89.6\n",
      "\tMinibatch loss at epoch 8000: 0.3053362965583801\n",
      "\tMinibatch accuracy: 95.9\n",
      "Test accuracy: 88.2\n",
      "\tMinibatch loss at epoch 9000: 0.2746383547782898\n",
      "\tMinibatch accuracy: 99.0\n",
      "Test accuracy: 88.6\n",
      "\tMinibatch loss at epoch 10000: 0.29845672845840454\n",
      "\tMinibatch accuracy: 97.9\n",
      "Test accuracy: 88.0\n",
      "\tMinibatch loss at epoch 11000: 0.33019691705703735\n",
      "\tMinibatch accuracy: 94.8\n",
      "Test accuracy: 88.3\n",
      "Final Test accuracy: 87.6\n",
      "time taken: 10.0 minutes 22.7 seconds\n"
     ]
    }
   ],
   "source": [
    "Run_Session(12000, 'DeepNN',0.8)\n",
    "\n",
    "#pdb.set_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic CNN model made\n"
     ]
    }
   ],
   "source": [
    "# single layer CNN\n",
    "num_labels = 4\n",
    "\n",
    "batch_size = 97\n",
    "check_size = 582\n",
    "feature_size = 193\n",
    "patch_size = 10\n",
    "num_channels = 1\n",
    "depth1 = 64\n",
    "\n",
    "num_hidden = 1050\n",
    "\n",
    "beta = 0.001\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    tf_data = tf.placeholder(tf.float32, shape=(None, feature_size),name='input_data')\n",
    "    train_labels = tf.placeholder(tf.float32, shape=(None, num_labels))\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # Variables.\n",
    "    layer1_weights = weight_variable([1, patch_size, 1, depth1])\n",
    "    layer1_biases = bias_variable([depth1])\n",
    "    layer2_weights = weight_variable([(feature_size//2 + 1)* depth1, num_hidden])\n",
    "    layer2_biases = bias_variable([num_hidden])\n",
    "    layer3_weights = weight_variable([num_hidden, num_labels])\n",
    "    layer3_biases = bias_variable([num_labels])\n",
    "\n",
    "    \n",
    "    # Model with dropout\n",
    "    def model(data, distort=None, proba=keep_prob):\n",
    "        \n",
    "        #if distort is not None:\n",
    "        #    data = tf.image.random_brightness(data, 0.9, seed=58)\n",
    "        #    data = tf.image.random_contrast(data, 0.1, 1.9, seed=58)\n",
    "        \n",
    "        # Convolution\n",
    "        conv1 = tf.nn.conv2d(data, layer1_weights, [1, 1, 2, 1] , padding='SAME') + layer1_biases\n",
    "        pooled1 = tf.nn.max_pool(tf.nn.relu(conv1), ksize=[1, 1, 2, 1],strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "        # Fully Connected Layer\n",
    "        shape = pooled1.get_shape().as_list()\n",
    "        reshape = tf.reshape(pooled1, [-1, shape[1] * shape[2] * shape[3]])\n",
    "        full2 = tf.nn.relu(tf.matmul(reshape, layer2_weights) + layer2_biases)\n",
    "        \n",
    "        # Dropout\n",
    "        full2 = tf.nn.dropout(full2, proba)\n",
    "        \n",
    "        return tf.matmul(full2, layer3_weights) + layer3_biases\n",
    "\n",
    "    # Training computation.\n",
    "    logits = model(tf.expand_dims(tf.expand_dims(tf_data, [-1]), 1), distort=True, proba=keep_prob)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=train_labels) +\n",
    "                         beta*tf.nn.l2_loss(layer1_weights) +\n",
    "                         beta*tf.nn.l2_loss(layer1_biases) +\n",
    "                         beta*tf.nn.l2_loss(layer2_weights) +\n",
    "                         beta*tf.nn.l2_loss(layer2_biases) +\n",
    "                         beta*tf.nn.l2_loss(layer3_weights) +\n",
    "                         beta*tf.nn.l2_loss(layer3_biases))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    prediction = tf.nn.softmax(logits)\n",
    "    test_prediction = tf.nn.softmax(model(tf.expand_dims(tf.expand_dims(tf_data, [-1]), 1), proba=1.0),name='op_to_restore')  \n",
    "    print('Basic CNN model made')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "\tMinibatch loss at epoch 0: 1.6506565809249878\n",
      "\tMinibatch accuracy: 20.6\n",
      "Test accuracy: 26.4\n",
      "\tMinibatch loss at epoch 1000: 0.26065221428871155\n",
      "\tMinibatch accuracy: 93.8\n",
      "Test accuracy: 88.7\n",
      "\tMinibatch loss at epoch 2000: 0.20331141352653503\n",
      "\tMinibatch accuracy: 94.8\n",
      "Test accuracy: 89.5\n",
      "\tMinibatch loss at epoch 3000: 0.13751330971717834\n",
      "\tMinibatch accuracy: 97.9\n",
      "Test accuracy: 88.5\n",
      "\tMinibatch loss at epoch 4000: 0.15481409430503845\n",
      "\tMinibatch accuracy: 94.8\n",
      "Test accuracy: 89.3\n",
      "\tMinibatch loss at epoch 5000: 0.12044704705476761\n",
      "\tMinibatch accuracy: 96.9\n",
      "Test accuracy: 90.0\n",
      "\tMinibatch loss at epoch 6000: 0.1327853947877884\n",
      "\tMinibatch accuracy: 96.9\n",
      "Test accuracy: 89.7\n",
      "\tMinibatch loss at epoch 7000: 0.13085632026195526\n",
      "\tMinibatch accuracy: 96.9\n",
      "Test accuracy: 89.8\n",
      "\tMinibatch loss at epoch 8000: 0.09798742830753326\n",
      "\tMinibatch accuracy: 97.9\n",
      "Test accuracy: 90.0\n",
      "\tMinibatch loss at epoch 9000: 0.12513801455497742\n",
      "\tMinibatch accuracy: 96.9\n",
      "Test accuracy: 90.3\n",
      "Final Test accuracy: 88.3\n",
      "time taken: 19.0 minutes 24.4 seconds\n"
     ]
    }
   ],
   "source": [
    "Run_Session(10000, 'CNN', .8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[12,7])\n",
    "plt.title('Accuracy for 193Feature set by epoch', fontsize=20)\n",
    "plt.plot(np.array(acc_over_time['DeepNN'])[:,0], np.array(acc_over_time['DeepNN'])[:,1], label='DeepNN')\n",
    "plt.plot(np.array(acc_over_time['CNN'])[:,0], np.array(acc_over_time['CNN'])[:,1], label='CNN')\n",
    "plt.xlabel('Epoch', fontsize=18)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.ylabel('Accuracy (%)', fontsize=18)\n",
    "plt.ylim([0, 100])\n",
    "plt.xlim([0, 10000])\n",
    "plt.legend(loc=\"lower right\", fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotROC(test_labels, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numeralize labels\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "test_batch = le.fit_transform(test['label'])\n",
    "\n",
    "\n",
    "def confusion(true, predicted):\n",
    "    matrix = np.zeros([4,4])\n",
    "    pred = predicted.reshape((predicted.shape[0]//4, 4))\n",
    "\n",
    "    count = 0\n",
    "    for lab in test_batch:\n",
    "        matrix[lab] += pred[count].round()\n",
    "        count += 1\n",
    "        \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = confusion(test_batch, test_preds['DeepNN'])\n",
    "\n",
    "plt.figure(figsize=[10,10])\n",
    "plt.imshow(matrix, cmap='hot', interpolation='nearest',  vmin=0, vmax=100)\n",
    "plt.colorbar()\n",
    "plt.title('DeepNN Confusion Map', fontsize=18)\n",
    "plt.ylabel('Actual', fontsize=18)\n",
    "plt.xlabel('Predicted', fontsize=18)\n",
    "plt.grid(b=False)\n",
    "plt.yticks(range(4), le.classes_, fontsize=14)\n",
    "plt.xticks(range(4), le.classes_, fontsize=14, rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "matrix = confusion(test_batch, test_preds['CNN'])\n",
    "\n",
    "fig = plt.figure(figsize=[10,10])\n",
    "plt.imshow(matrix, cmap='hot', interpolation='none', vmin=0, vmax=100)\n",
    "plt.colorbar()\n",
    "plt.title('CNN Confusion Map', fontsize=18)\n",
    "plt.ylabel('Actual', fontsize=18)\n",
    "plt.xlabel('Predicted', fontsize=18)\n",
    "plt.grid(b=False)\n",
    "plt.yticks(range(4), le.classes_, fontsize=14)\n",
    "plt.xticks(range(4), le.classes_, fontsize=14, rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "with tf.Session() as sess:    \n",
    "    saver = tf.train.import_meta_graph('C:/Users/kkhalid/project_kk/model-ckpt/trained_py_variation_6k.ckpt.meta')\n",
    "    saver.restore(sess,tf.train.latest_checkpoint('C:/Users/kkhalid/project_kk/model-ckpt/checkpoint.ckpt/checkpoint.ckpt',latest_filename=None))\n",
    "    print(sess.run('w1:0'))\n",
    "##Model has been restored. Above statement will print the saved value of w1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
